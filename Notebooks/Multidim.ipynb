{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Assume the modified tree functions (TreeNode, build_tree_from_paths, etc.) have been defined or imported.\n",
    "# For example, TreeNode can remain unchanged:\n",
    "class TreeNode:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.children = []\n",
    "    def add_child(self, child_node, probability):\n",
    "        self.children.append((child_node, probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# A helper to convert a value to a hashable type.\n",
    "def to_hashable(x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return tuple(x.tolist())\n",
    "    return x\n",
    "\n",
    "# Modified pad_paths: if the node value is a vector, pad with a zero vector.\n",
    "def pad_paths(paths, pad_value=None):\n",
    "    max_length = max(len(path) for path in paths)\n",
    "    # Determine pad_value based on the first element if not provided.\n",
    "    if pad_value is None:\n",
    "        first_val = paths[0][0]\n",
    "        if isinstance(first_val, (np.ndarray, list)):\n",
    "            first_arr = np.array(first_val)\n",
    "            pad_value = np.zeros(first_arr.shape)\n",
    "        else:\n",
    "            pad_value = 0\n",
    "    padded = np.array([path + [pad_value] * (max_length - len(path)) for path in paths])\n",
    "    return padded\n",
    "\n",
    "# Modified build_tree_from_paths: works with vector node values.\n",
    "def build_tree_from_paths(sample_paths, weights):\n",
    "    \"\"\"\n",
    "    Builds a weighted tree from sample paths. Each path is assumed to be a list/array\n",
    "    where each element can be a scalar (d=1) or a vector (d>1).\n",
    "    \"\"\"\n",
    "    # Use to_hashable for checking the starting value.\n",
    "    start_value = sample_paths[0][0]\n",
    "    start_key = to_hashable(start_value)\n",
    "    for path in sample_paths:\n",
    "        if to_hashable(path[0]) != start_key:\n",
    "            raise ValueError(\"All sample paths must have the same value at time step 0.\")\n",
    "    \n",
    "    total_weight = sum(weights)\n",
    "    if abs(total_weight - 1.0) > 1e-6:\n",
    "        raise ValueError(\"The sum of weights must equal 1. Got sum(weights) = {}\".format(total_weight))\n",
    "    \n",
    "    tree_dict = {\"value\": start_value, \"children\": {}}\n",
    "    for path, path_weight in zip(sample_paths, weights):\n",
    "        current = tree_dict\n",
    "        for value in path[1:]:\n",
    "            key = to_hashable(value)\n",
    "            if key not in current[\"children\"]:\n",
    "                current[\"children\"][key] = {\"node\": {\"value\": value, \"children\": {}}, \"weight\": 0.0}\n",
    "            current[\"children\"][key][\"weight\"] += path_weight\n",
    "            current = current[\"children\"][key][\"node\"]\n",
    "    \n",
    "    def convert_tree_dict(node_dict):\n",
    "        node = TreeNode(node_dict[\"value\"])\n",
    "        children = node_dict[\"children\"]\n",
    "        if children:\n",
    "            total = sum(child_info[\"weight\"] for child_info in children.values())\n",
    "            for child_key, child_info in children.items():\n",
    "                child_node = convert_tree_dict(child_info[\"node\"])\n",
    "                probability = child_info[\"weight\"] / total if total > 0 else 0\n",
    "                node.add_child(child_node, probability)\n",
    "        return node\n",
    "\n",
    "    return convert_tree_dict(tree_dict)\n",
    "\n",
    "# Modified find_node_by_path: uses to_hashable for vector comparisons.\n",
    "def find_node_by_path(node, path):\n",
    "    current_node = node\n",
    "    for value in path[1:]:\n",
    "        key = to_hashable(value)\n",
    "        matching_children = [\n",
    "            child for child, _ in current_node.children if to_hashable(child.value) == key\n",
    "        ]\n",
    "        if not matching_children:\n",
    "            print(f\"Invalid path: No child with value {value} under node with value {current_node.value}.\")\n",
    "            return None\n",
    "        if len(matching_children) > 1:\n",
    "            print(f\"Warning: Multiple children with value {value} found under node with value {current_node.value}.\")\n",
    "        current_node = matching_children[0]\n",
    "    return current_node\n",
    "\n",
    "# Modified uniform_empirical_grid_measure: supports both 2D (d=1) and 3D (d>1) sample paths.\n",
    "def uniform_empirical_grid_measure(data, delta_n=None, use_weights=False):\n",
    "    # If data is 2D, each path is a 1D sequence (d = 1).\n",
    "    if data.ndim == 2:\n",
    "        num_path, t = data.shape\n",
    "        if delta_n is None:\n",
    "            delta_n = 1 / (num_path ** (1 / t))\n",
    "        grid_func = lambda x: np.floor(x / delta_n + 0.5) * delta_n\n",
    "        quantized_data = grid_func(data)\n",
    "        quantized_data[:, 0] = data[:, 0]\n",
    "        if not use_weights:\n",
    "            return quantized_data\n",
    "        else:\n",
    "            unique_paths, indices, counts = np.unique(\n",
    "                quantized_data, axis=0, return_inverse=True, return_counts=True\n",
    "            )\n",
    "            weights = counts / num_path\n",
    "            return unique_paths, weights\n",
    "    # If data is 3D, each path has shape (T+1, d).\n",
    "    elif data.ndim == 3:\n",
    "        num_path, t, d = data.shape\n",
    "        if delta_n is None:\n",
    "            # For multi-dimensional paths, one may adjust the exponent as needed.\n",
    "            delta_n = 1 / (num_path ** (1 / t))\n",
    "        grid_func = lambda x: np.floor(x / delta_n + 0.5) * delta_n\n",
    "        quantized_data = grid_func(data)\n",
    "        quantized_data[:, 0, :] = data[:, 0, :]\n",
    "        if not use_weights:\n",
    "            return quantized_data\n",
    "        else:\n",
    "            # Convert each sample path (2D array) into a hashable tuple of tuples.\n",
    "            quantized_paths = [tuple(map(tuple, quantized_data[i])) for i in range(num_path)]\n",
    "            # Preserve the original order.\n",
    "            unique_paths_list = []\n",
    "            counts_list = []\n",
    "            for p in quantized_paths:\n",
    "                if p in unique_paths_list:\n",
    "                    counts_list[unique_paths_list.index(p)] += 1\n",
    "                else:\n",
    "                    unique_paths_list.append(p)\n",
    "                    counts_list.append(1)\n",
    "            weights = np.array(counts_list) / num_path\n",
    "            # Convert unique paths back to numpy arrays.\n",
    "            unique_paths = np.array([np.array(up) for up in unique_paths_list])\n",
    "            return unique_paths, weights\n",
    "    else:\n",
    "        raise ValueError(\"Data must be either 2D (d=1) or 3D (d>1).\")\n",
    "\n",
    "# Modified compute_distance_matrix_at_depth: if node values are vectors, use Euclidean norm.\n",
    "def compute_distance_matrix_at_depth(tree_1_root, tree_2_root, depth, power):\n",
    "    nodes_at_depth_tree1 = get_nodes_at_depth(tree_1_root, depth)\n",
    "    nodes_at_depth_tree2 = get_nodes_at_depth(tree_2_root, depth)\n",
    "    arr1 = pad_paths(nodes_at_depth_tree1)  # shape: (m, L) or (m, L, d)\n",
    "    arr2 = pad_paths(nodes_at_depth_tree2)  # shape: (n, L) or (n, L, d)\n",
    "    \n",
    "    if arr1.ndim == 2:  # d = 1 (scalar values)\n",
    "        diff = np.abs(arr1[:, None, :] - arr2[None, :, :]) ** power\n",
    "        distance_matrix = diff.sum(axis=2)\n",
    "    elif arr1.ndim == 3:  # d > 1 (vector values)\n",
    "        diff = arr1[:, None, :, :] - arr2[None, :, :, :]  # shape (m, n, L, d)\n",
    "        diff_norm = np.linalg.norm(diff, axis=3) ** power  # norm over the d-dimensions at each time step\n",
    "        distance_matrix = diff_norm.sum(axis=2)\n",
    "    return distance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compare two node values.\n",
    "def value_equal(v1, v2):\n",
    "    return to_hashable(v1) == to_hashable(v2)\n",
    "\n",
    "def get_nodes_at_depth(tree_root, depth):\n",
    "    \"\"\"Collect all nodes at a specific depth from the root.\n",
    "       Returns each path as a list of node values (which can be scalars or vectors).\"\"\"\n",
    "    nodes = []\n",
    "\n",
    "    def traverse(node, path, current_depth):\n",
    "        if current_depth == depth:\n",
    "            nodes.append(path + [node.value])\n",
    "            return\n",
    "        for child, _ in node.children:\n",
    "            traverse(child, path + [node.value], current_depth + 1)\n",
    "\n",
    "    traverse(tree_root, [], 0)\n",
    "    return nodes\n",
    "\n",
    "\n",
    "def get_paths_to_leaves(tree_root, max_depth):\n",
    "    \"\"\"Generate all paths from the root to each leaf node up to a specified depth.\n",
    "       Each path is returned as a list of node values.\"\"\"\n",
    "    paths = []\n",
    "\n",
    "    def traverse(node, path, depth):\n",
    "        if depth == max_depth or not node.children:\n",
    "            paths.append(path + [node.value])\n",
    "            return\n",
    "        for child, _ in node.children:\n",
    "            traverse(child, path + [node.value], depth + 1)\n",
    "\n",
    "    traverse(tree_root, [], 0)\n",
    "    return paths\n",
    "\n",
    "\n",
    "def get_node_from_path(tree_root, path):\n",
    "    \"\"\"Given a root node and a path (list of values), return the node at the end of the path.\n",
    "       Uses value_equal() to compare node values, ensuring consistency for both d=1 and d>1.\"\"\"\n",
    "    current_node = tree_root\n",
    "    for value in path[1:]:\n",
    "        current_node = next(\n",
    "            child for child, _ in current_node.children if value_equal(child.value, value)\n",
    "        )\n",
    "    return current_node\n",
    "\n",
    "\n",
    "def compute_marginal_probabilities_for_subset(\n",
    "    node1_path, node2_path, tree_1_root, tree_2_root\n",
    "):\n",
    "    \"\"\"Compute marginal probabilities for the direct successors of node1 and node2.\n",
    "       It extracts the nodes at the end of the paths and then builds their successor paths.\"\"\"\n",
    "    node1 = get_node_from_path(tree_1_root, node1_path)\n",
    "    node2 = get_node_from_path(tree_2_root, node2_path)\n",
    "\n",
    "    successors_node1 = [\n",
    "        (node1_path + [child.value], prob) for child, prob in node1.children\n",
    "    ]\n",
    "    successors_node2 = [\n",
    "        (node2_path + [child.value], prob) for child, prob in node2.children\n",
    "    ]\n",
    "\n",
    "    pi_ratios = [prob for _, prob in successors_node1]\n",
    "    pi_tilde_ratios = [prob for _, prob in successors_node2]\n",
    "\n",
    "    return pi_ratios, pi_tilde_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_paths(tree_root):\n",
    "    \"\"\"\n",
    "    Extracts all possible paths from the root to the leaves of the tree\n",
    "    along with their associated probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - tree_root (TreeNode): The root node of the tree.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing:\n",
    "        - paths_array (np.ndarray): 2D array where each row is a path.\n",
    "        - probabilities_array (np.ndarray): 1D array of path probabilities.\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    probabilities = []\n",
    "\n",
    "    def traverse(node, current_path, current_prob):\n",
    "        \"\"\"\n",
    "        Recursively traverses the tree to collect paths and probabilities.\n",
    "\n",
    "        Parameters:\n",
    "        - node (TreeNode): The current node being traversed.\n",
    "        - current_path (list): The path taken to reach the current node.\n",
    "        - current_prob (float): The cumulative probability of the current path.\n",
    "        \"\"\"\n",
    "        new_path = current_path + [node.value]\n",
    "\n",
    "        if not node.children:\n",
    "            paths.append(new_path)\n",
    "            probabilities.append(current_prob)\n",
    "            return\n",
    "\n",
    "        for child, prob in node.children:\n",
    "            traverse(child, new_path, current_prob * prob)\n",
    "\n",
    "    # Initialize traversal from the root\n",
    "    traverse(tree_root, [], 1.0)\n",
    "\n",
    "    # Convert lists to NumPy arrays\n",
    "    paths_array = np.array(paths)\n",
    "    probabilities_array = np.array(probabilities)\n",
    "\n",
    "    return [paths_array, probabilities_array]\n",
    "\n",
    "\n",
    "def display_tree_data(paths_weights, tree_name):\n",
    "    \"\"\"\n",
    "    Displays the paths and their associated probabilities for a given tree.\n",
    "\n",
    "    Parameters:\n",
    "    - paths_weights (tuple): A tuple containing paths and probabilities.\n",
    "    - tree_name (str): The name of the tree for display purposes.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{tree_name} (Path and Weight Format):\")\n",
    "    print(\"Paths:\")\n",
    "    print(paths_weights[0])\n",
    "    print(\"Weights:\")\n",
    "    print(paths_weights[1])\n",
    "\n",
    "\n",
    "def get_depth(tree_root):\n",
    "    \"\"\"\n",
    "    Calculates the depth (height) of the tree, starting at 0 for the root node.\n",
    "\n",
    "    Parameters:\n",
    "    - tree_root (TreeNode): The root node of the tree.\n",
    "\n",
    "    Returns:\n",
    "    - int: The depth of the tree.\n",
    "    \"\"\"\n",
    "    if tree_root is None:\n",
    "        return -1  # Assuming an empty tree has a depth of -1 (convenient for us)\n",
    "\n",
    "    # If the node has no children, its depth is 0\n",
    "    if not tree_root.children:\n",
    "        return 0\n",
    "\n",
    "    # Since all paths have the same depth, we can check just one path\n",
    "    first_child, _ = tree_root.children[0]\n",
    "    return 1 + get_depth(first_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nested_optimal_transport_loop(\n",
    "    tree1_root, tree2_root, max_depth, method, lambda_reg, power\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the nested optimal transport plan between two trees.\n",
    "\n",
    "    Parameters:\n",
    "    - tree1_root (TreeNode): Root of the first tree.\n",
    "    - tree2_root (TreeNode): Root of the second tree.\n",
    "    - max_depth (int): Maximum depth to compute.\n",
    "    - method (str): Solver method: \"Sinkhorn\", \"solver_lp\", or \"solver_pot\".\n",
    "    - lambda_reg (float): Regularization parameter for Sinkhorn (only used if method=\"Sinkhorn\").\n",
    "\n",
    "    Returns:\n",
    "    - float: Computed nested distance.\n",
    "    - dict: Dictionary of probability matrices for each step.\n",
    "    \"\"\"\n",
    "    if method == \"Sinkhorn\" and lambda_reg <= 0:\n",
    "        raise ValueError(\"Lambda must be positive when using Sinkhorn iteration.\")\n",
    "    elif method not in (\n",
    "        \"solver_lp\",\n",
    "        \"solver_pot\",\n",
    "        \"Sinkhorn\",\n",
    "        \"solver_sinkhorn\",\n",
    "        \"solver_lp_pot\",\n",
    "        \"solver_pot_sinkhorn\",\n",
    "        \"solver_jax\",\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Method must be one of 'Sinkhorn', 'solver_lp', 'solver_jax', 'solver_lp_pot', 'solver_pot_sinkhorn' or 'solver_pot'.\"\n",
    "        )\n",
    "\n",
    "    probability_matrices = {}\n",
    "    full_distance_matrix = compute_distance_matrix_at_depth(\n",
    "        tree1_root, tree2_root, max_depth, power\n",
    "    )\n",
    "\n",
    "    for depth in range(max_depth - 1, -1, -1):\n",
    "        paths_tree1 = get_nodes_at_depth(tree1_root, depth)\n",
    "        paths_tree2 = get_nodes_at_depth(tree2_root, depth)\n",
    "\n",
    "        nodes_tree1 = [find_node_by_path(tree1_root, path) for path in paths_tree1]\n",
    "        nodes_tree2 = [find_node_by_path(tree2_root, path) for path in paths_tree2]\n",
    "\n",
    "        children_count_tree1 = [len(node.children) for node in nodes_tree1 if node]\n",
    "        children_count_tree2 = [len(node.children) for node in nodes_tree2 if node]\n",
    "\n",
    "        updated_distance_matrix = np.zeros((len(paths_tree1), len(paths_tree2)))\n",
    "\n",
    "        for i, path1 in enumerate(paths_tree1):\n",
    "            for j, path2 in enumerate(paths_tree2):\n",
    "                step_name = (depth, to_hashable(path1[-1]), to_hashable(path2[-1]))\n",
    "\n",
    "                start_row, end_row = sum(children_count_tree1[:i]), sum(\n",
    "                    children_count_tree1[: i + 1]\n",
    "                )\n",
    "                start_col, end_col = sum(children_count_tree2[:j]), sum(\n",
    "                    children_count_tree2[: j + 1]\n",
    "                )\n",
    "                sub_matrix = full_distance_matrix[start_row:end_row, start_col:end_col]\n",
    "\n",
    "                pi_ratios, pi_tilde_ratios = compute_marginal_probabilities_for_subset(\n",
    "                    path1, path2, tree1_root, tree2_root\n",
    "                )\n",
    "\n",
    "                if method == \"Sinkhorn\":\n",
    "                    probability_matrix = Sinkhorn_iteration(\n",
    "                        sub_matrix,\n",
    "                        pi_ratios,\n",
    "                        pi_tilde_ratios,\n",
    "                        stopping_criterion=1e-4,\n",
    "                        lambda_reg=lambda_reg,\n",
    "                    )\n",
    "                elif method == \"solver_lp_pot\":\n",
    "                    probability_matrix = solver_lp_pot(\n",
    "                        sub_matrix, pi_ratios, pi_tilde_ratios\n",
    "                    )\n",
    "                elif method == \"solver_lp\":\n",
    "                    probability_matrix = solver_lp(\n",
    "                        sub_matrix, pi_ratios, pi_tilde_ratios\n",
    "                    )\n",
    "                elif method == \"solver_jax\":\n",
    "                    probability_matrix = solver_jax(\n",
    "                        sub_matrix, pi_ratios, pi_tilde_ratios, epsilon=(1 / lambda_reg)\n",
    "                    )\n",
    "                elif method == \"solver_pot_sinkhorn\":\n",
    "                    probability_matrix = solver_pot_sinkhorn(\n",
    "                        sub_matrix, pi_ratios, pi_tilde_ratios, epsilon=(1 / lambda_reg)\n",
    "                    )\n",
    "                elif method == \"solver_pot_1D\":\n",
    "                    probability_matrix = solver_pot_1D(\n",
    "                        sub_matrix, pi_ratios, pi_tilde_ratios\n",
    "                    )\n",
    "                else:\n",
    "                    probability_matrix = solver_pot(\n",
    "                        sub_matrix, pi_ratios, pi_tilde_ratios\n",
    "                    )\n",
    "\n",
    "                cost = np.sum(probability_matrix * sub_matrix)\n",
    "                probability_matrices[step_name] = probability_matrix\n",
    "                updated_distance_matrix[i, j] = cost\n",
    "\n",
    "        full_distance_matrix = updated_distance_matrix\n",
    "\n",
    "    return full_distance_matrix[0][0], probability_matrices\n",
    "\n",
    "\n",
    "# DO NOT USE FOR BIG PROBLEMS\n",
    "def compute_final_probability_matrix(\n",
    "    probability_matrices, tree1_root, tree2_root, max_depth\n",
    "):\n",
    "    \"\"\"\n",
    "    Combines probability matrices along all paths to compute the final probability matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - probability_matrices (dict): Dictionary of probability matrices for each step.\n",
    "    - tree1_root (TreeNode): Root of the first tree.\n",
    "    - tree2_root (TreeNode): Root of the second tree.\n",
    "    - max_depth (int): Maximum depth considered.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Final probability matrix representing nested distance.\n",
    "    \"\"\"\n",
    "    paths_tree1 = get_paths_to_leaves(tree1_root, max_depth)\n",
    "    paths_tree2 = get_paths_to_leaves(tree2_root, max_depth)\n",
    "    final_prob_matrix = np.zeros((len(paths_tree1), len(paths_tree2)))\n",
    "\n",
    "    for i, path1 in enumerate(paths_tree1):\n",
    "        for j, path2 in enumerate(paths_tree2):\n",
    "            probability = 1.0\n",
    "            for depth in range(max_depth):\n",
    "                if depth >= len(path1) or depth >= len(path2):\n",
    "                    break\n",
    "                step_name = (depth, path1[depth], path2[depth])\n",
    "                prob_matrix = probability_matrices.get(step_name, None)\n",
    "                if prob_matrix is None or prob_matrix.size == 0:\n",
    "                    probability = 0\n",
    "                    break\n",
    "                next_node1 = path1[depth + 1] if depth + 1 < len(path1) else None\n",
    "                next_node2 = path2[depth + 1] if depth + 1 < len(path2) else None\n",
    "                successors_node1 = [\n",
    "                    child[-1]\n",
    "                    for child in get_paths_to_leaves(tree1_root, depth + 1)\n",
    "                    if child[:-1] == path1[: depth + 1]\n",
    "                ]\n",
    "                successors_node2 = [\n",
    "                    child[-1]\n",
    "                    for child in get_paths_to_leaves(tree2_root, depth + 1)\n",
    "                    if child[:-1] == path2[: depth + 1]\n",
    "                ]\n",
    "                try:\n",
    "                    index1 = successors_node1.index(next_node1)\n",
    "                    index2 = successors_node2.index(next_node2)\n",
    "                except ValueError:\n",
    "                    probability = 0\n",
    "                    break\n",
    "                probability *= prob_matrix[index1, index2]\n",
    "            final_prob_matrix[i, j] = probability\n",
    "    return final_prob_matrix\n",
    "\n",
    "\n",
    "def compute_nested_distance(\n",
    "    tree1_root,\n",
    "    tree2_root,\n",
    "    max_depth,\n",
    "    return_matrix=False,\n",
    "    method=\"solver_lp\",\n",
    "    lambda_reg=0,\n",
    "    power=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the nested Wasserstein distance between two trees.\n",
    "\n",
    "    Parameters:\n",
    "    - tree1_root (TreeNode): Root of the first tree.\n",
    "    - tree2_root (TreeNode): Root of the second tree.\n",
    "    - max_depth (int): Maximum depth to compute.\n",
    "    - return_matrix (bool): If True, returns the final probability matrix.\n",
    "    - method (str): Solver method: \"Sinkhorn\", \"solver_lp\", or \"solver_pot\".\n",
    "    - lambda_reg (float): Regularization parameter for Sinkhorn.\n",
    "\n",
    "    Returns:\n",
    "    - float: Computed nested distance.\n",
    "    - np.ndarray (optional): Final probability matrix if return_matrix is True.\n",
    "    \"\"\"\n",
    "    distance, probability_matrices = nested_optimal_transport_loop(\n",
    "        tree1_root, tree2_root, max_depth, method, lambda_reg, power\n",
    "    )\n",
    "\n",
    "    if return_matrix:\n",
    "        final_prob_matrix = compute_final_probability_matrix(\n",
    "            probability_matrices, tree1_root, tree2_root, max_depth\n",
    "        )\n",
    "        return distance, final_prob_matrix\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ot\n",
    "\n",
    "def solver_lp_pot(distance_matrix_subset, pi_ratios, pi_tilde_ratios, reg=1e-2):\n",
    "    \"\"\"\n",
    "    Solve for the optimal transport plan using the POT library's (fast!) EMD solver.\n",
    "\n",
    "    Parameters:\n",
    "    - distance_matrix_subset (np.ndarray): A 2D cost matrix.\n",
    "    - pi_ratios (np.ndarray): 1D source distribution (row marginals).\n",
    "    - pi_tilde_ratios (np.ndarray): 1D target distribution (column marginals).\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: The optimal transport plan (probability matrix).\n",
    "    \"\"\"\n",
    "    pi_ratios = np.array(pi_ratios, dtype=np.float64)\n",
    "    pi_tilde_ratios = np.array(pi_tilde_ratios, dtype=np.float64)\n",
    "\n",
    "    return ot.lp.emd(pi_ratios, pi_tilde_ratios, distance_matrix_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nested distance: 18.467465828293193\n",
      "Computation time: 184.9159 seconds\n"
     ]
    }
   ],
   "source": [
    "# Set normalization flag (set to False to use L0 and M0 directly)\n",
    "normalize = False\n",
    "\n",
    "# Define factor matrices (6x6) for d=2, T=3\n",
    "L0 = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [1, 2, 0, 0],\n",
    "    [1, 2, 3, 0],\n",
    "    [7, 5, 4, 9]\n",
    "])\n",
    "A0 = L0 @ L0.T\n",
    "L = L0 / np.sqrt(np.trace(A0)) if normalize else L0\n",
    "A = L @ L.T\n",
    "\n",
    "M0 = np.array([\n",
    "    [2, 0, 0, 0],\n",
    "    [3, 1, 0, 0],\n",
    "    [1, 4, 2, 0],\n",
    "    [8, 5, 3, 7]\n",
    "])\n",
    "B0 = M0 @ M0.T\n",
    "M = M0 / np.sqrt(np.trace(B0)) if normalize else M0\n",
    "B = M @ M.T\n",
    "\n",
    "# Set dimension parameters: for d=2, T=3 (thus total dimension = 6)\n",
    "d = 2\n",
    "T = 2\n",
    "dim = d * T  # 4\n",
    "\n",
    "n_sample_plot = 600  # number of sample paths\n",
    "\n",
    "X_paths = []\n",
    "Y_paths = []\n",
    "for _ in range(n_sample_plot):\n",
    "    # Generate noise as a vector in R^{dim}\n",
    "    noise1 = np.random.normal(size=(dim,))  # shape: (6,)\n",
    "    noise2 = np.random.normal(size=(dim,))  # shape: (6,)\n",
    "    # Obtain increments: these are vectors in R^{dim} (6,)\n",
    "    X_increments = L @ noise1  # shape: (6,)\n",
    "    Y_increments = M @ noise2  # shape: (6,)\n",
    "    # Reshape into (T, d) = (3, 2)\n",
    "    X_increments = X_increments.reshape((T, d))\n",
    "    Y_increments = Y_increments.reshape((T, d))\n",
    "    # (Optionally, if you still want to prepend a zero step, do it here.)\n",
    "    X_sample = np.vstack([np.zeros((1, d)), X_increments])\n",
    "    Y_sample = np.vstack([np.zeros((1, d)), Y_increments])\n",
    "\n",
    "    X_paths.append(X_sample)\n",
    "    Y_paths.append(Y_sample)\n",
    "    \n",
    "X_paths = np.array(X_paths)  # shape: (160, 3, 2)\n",
    "Y_paths = np.array(Y_paths)  # shape: (160, 3, 2)\n",
    "\n",
    "# Adapt the empirical measure using uniform grid quantization.\n",
    "adapted_X, adapted_weights_X = uniform_empirical_grid_measure(X_paths, use_weights=True)\n",
    "adapted_Y, adapted_weights_Y = uniform_empirical_grid_measure(Y_paths, use_weights=True)\n",
    "\n",
    "# Build trees from the adapted paths.\n",
    "adapted_tree_1 = build_tree_from_paths(adapted_X, adapted_weights_X)\n",
    "adapted_tree_2 = build_tree_from_paths(adapted_Y, adapted_weights_Y)\n",
    "\n",
    "max_depth = get_depth(adapted_tree_1)\n",
    "start_time = time.time()\n",
    "\n",
    "# Compute the nested distance using your chosen optimal transport solver (here, \"solver_lp_pot\" is used).\n",
    "distance_pot = compute_nested_distance(\n",
    "    adapted_tree_1,\n",
    "    adapted_tree_2,\n",
    "    max_depth,\n",
    "    method=\"solver_lp_pot\",\n",
    "    return_matrix=False,\n",
    "    lambda_reg=0,\n",
    "    power=2,\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Nested distance:\", distance_pot)\n",
    "print(\"Computation time: {:.4f} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapted Wasserstein Squared Distance for custom Gaussian process: 16.32897915629934\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "notebooks_path = os.path.abspath(os.getcwd()) \n",
    "src_path = os.path.abspath(os.path.join(notebooks_path, \"../src\"))\n",
    "\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "from benchmark_value_gaussian.Comp_AWD2_Gaussian import *\n",
    "\n",
    "\n",
    "# Define zero mean vectors for both processes in R^(d*T)\n",
    "a = np.zeros(dim)\n",
    "b = np.zeros(dim)\n",
    "\n",
    "# Compute the adapted Wasserstein squared distance for the custom Gaussian process\n",
    "distance_aw2 = adapted_wasserstein_squared(a, A, b, B, d, T)\n",
    "print(\"Adapted Wasserstein Squared Distance for custom Gaussian process:\", distance_aw2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
