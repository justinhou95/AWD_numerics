{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed13e3f4-0ca0-4584-99fd-1b24c619cf21",
   "metadata": {},
   "source": [
    "This code comes: https://github.com/hanbingyan/FVIOT/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4edf8d7a-2b21-4a40-afa1-3159ddfd74c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(12345)\n",
    "np.random.seed(12345)\n",
    "torch.manual_seed(12345)\n",
    "# check gpu is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Config\n",
    "MEM_SIZE = 3000\n",
    "BATCH_SIZE = 128\n",
    "DISCOUNT = 1.0\n",
    "N_INSTANCE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec6d8b1-cb73-4321-8575-bceef17f2bfb",
   "metadata": {},
   "source": [
    "Utility functions for a more compact code and for the optimisation of the nerual networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af0eccd-6725-4242-b27f-667020e53863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch.nn as nn\n",
    "\n",
    "# def sinkhorn_knopp(mu, nu, C, reg, niter):\n",
    "#     K = np.exp(-C/C.max()/reg)\n",
    "#     u = np.ones((len(mu), ))\n",
    "#     for i in range(1, niter):\n",
    "#         v = nu/np.dot(K.T, u)\n",
    "#         u = mu/(np.dot(K, v))\n",
    "#     Pi = np.diag(u) @ K @ np.diag(v)\n",
    "#     return Pi\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition', ('time', 'x', 'y', 'value'))\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def clear(self):\n",
    "        self.memory.clear()\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "def optimize_model(policy_net, memory, optimizer, Trunc_flag):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    values_batch = torch.stack(batch.value)\n",
    "    x_batch = torch.stack(batch.x)\n",
    "    y_batch = torch.stack(batch.y)\n",
    "    time_batch = torch.stack(batch.time)\n",
    "\n",
    "    left_values = policy_net(time_batch, x_batch, y_batch)\n",
    "\n",
    "    # # Compute the expected Q values\n",
    "    Loss_fn = nn.SmoothL1Loss()\n",
    "    # Loss_fn = nn.MSELoss()\n",
    "    loss = Loss_fn(left_values, values_batch)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    if Trunc_flag:\n",
    "        for param in policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5008f3da-9a6a-49e4-aa09-8ab14e0b776e",
   "metadata": {},
   "source": [
    "The neural network at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04caccf1-804c-4832-bd22-cd0115d9b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nbimporter\n",
    "\n",
    "h = 8\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, x_dim, y_dim, T):\n",
    "        super(DQN, self).__init__()\n",
    "        self.T = T\n",
    "        self.linear1 = nn.Linear(x_dim+y_dim, h)\n",
    "        # self.linear1.weight.data.fill_(10.0)\n",
    "        # torch.nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        # torch.nn.init.zeros_(self.linear1.weight)\n",
    "        # torch.nn.init.zeros_(self.linear1.bias)\n",
    "        # self.bn = nn.BatchNorm1d(h)\n",
    "        self.linear2 = nn.Linear(h, h)\n",
    "        # torch.nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        # torch.nn.init.zeros_(self.linear2.bias)\n",
    "        # torch.nn.init.zeros_(self.linear2.weight)\n",
    "\n",
    "        # self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.linear3 = nn.Linear(h, 1)\n",
    "\n",
    "        self.linear5 = nn.Linear(2, 1)\n",
    "        # torch.nn.init.zeros_(self.linear5.bias)\n",
    "        # torch.nn.init.zeros_(self.linear5.weight)\n",
    "        # torch.nn.init.xavier_uniform_(self.linear5.weight)\n",
    "        self.linear6 = nn.Linear(2, 1)\n",
    "        # torch.nn.init.zeros_(self.linear6.bias)\n",
    "\n",
    "    def forward(self, time, x, y):\n",
    "        state = torch.cat((x, y), dim=1)\n",
    "        state = torch.relu(self.linear1(state))\n",
    "        # state = self.bn(state)\n",
    "        state = torch.relu(self.linear2(state))\n",
    "        # state = self.dropout(state)\n",
    "        state = torch.sigmoid(self.linear3(state))\n",
    "        time_f2 = torch.cat((self.T - time, (self.T - time)**2), dim=1)\n",
    "        time_f1 =  self.linear5(time_f2)\n",
    "        time_f2 = self.linear6(time_f2)\n",
    "        return state*time_f1 + time_f2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe5a484-11a1-499c-8eb0-eb1eee366357",
   "metadata": {},
   "source": [
    "The code to compute the Adapted Wasserstein distance (AW_2) between two brownian path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08fc842d-b1c8-4bd4-87e1-fe45a11a8664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample\n",
      "tensor([-0.9818])\n",
      "tensor([[-0.6633],\n",
      "        [-1.9240],\n",
      "        [ 0.6006],\n",
      "        [-1.7176],\n",
      "        [-2.3229],\n",
      "        [-1.0638],\n",
      "        [-2.0598],\n",
      "        [-2.6968],\n",
      "        [-0.5009],\n",
      "        [-2.6075],\n",
      "        [ 0.0511],\n",
      "        [-2.3122],\n",
      "        [-0.1308],\n",
      "        [ 0.1243],\n",
      "        [-1.7932],\n",
      "        [-2.2979],\n",
      "        [-1.3407],\n",
      "        [-0.5643],\n",
      "        [-0.9903],\n",
      "        [-1.1090],\n",
      "        [-0.1600],\n",
      "        [-1.8139],\n",
      "        [-2.1564],\n",
      "        [-0.1443],\n",
      "        [-0.5334],\n",
      "        [-1.2841],\n",
      "        [-1.0870],\n",
      "        [-1.2371],\n",
      "        [ 0.2489],\n",
      "        [-1.6783],\n",
      "        [-2.6822],\n",
      "        [-0.2962],\n",
      "        [-1.5252],\n",
      "        [ 0.1537],\n",
      "        [-1.5769],\n",
      "        [-0.5280],\n",
      "        [-2.9334],\n",
      "        [-2.0478],\n",
      "        [-2.6218],\n",
      "        [ 0.4389],\n",
      "        [-0.1735],\n",
      "        [-1.0208],\n",
      "        [-1.7519],\n",
      "        [-0.8124],\n",
      "        [ 0.3038],\n",
      "        [-1.3702],\n",
      "        [-0.1798],\n",
      "        [-1.1531],\n",
      "        [-0.3081],\n",
      "        [-0.9114]])\n",
      "Time step 8 Loss 0.0006095532091421774\n",
      "sample\n",
      "tensor([-0.6431])\n",
      "tensor([[-1.2813],\n",
      "        [-0.6190],\n",
      "        [-0.2447],\n",
      "        [-0.8959],\n",
      "        [-0.3539],\n",
      "        [-1.5084],\n",
      "        [ 0.8459],\n",
      "        [-1.5297],\n",
      "        [ 0.1396],\n",
      "        [-1.8513],\n",
      "        [-1.0708],\n",
      "        [-0.5204],\n",
      "        [-0.8040],\n",
      "        [-0.0329],\n",
      "        [-0.6323],\n",
      "        [ 0.6584],\n",
      "        [ 1.3023],\n",
      "        [-0.5388],\n",
      "        [-1.3640],\n",
      "        [-0.2890],\n",
      "        [ 0.3425],\n",
      "        [ 0.5197],\n",
      "        [-2.6281],\n",
      "        [ 0.9189],\n",
      "        [-0.6616],\n",
      "        [ 0.1965],\n",
      "        [ 1.6443],\n",
      "        [-1.5095],\n",
      "        [-1.6095],\n",
      "        [-1.9525],\n",
      "        [-1.0476],\n",
      "        [-1.9093],\n",
      "        [ 0.0421],\n",
      "        [-1.1062],\n",
      "        [-2.0453],\n",
      "        [-1.8573],\n",
      "        [-0.9120],\n",
      "        [-1.3262],\n",
      "        [-0.1776],\n",
      "        [-2.1013],\n",
      "        [ 0.5093],\n",
      "        [-0.1309],\n",
      "        [-0.1428],\n",
      "        [ 0.4896],\n",
      "        [ 0.0419],\n",
      "        [-0.3103],\n",
      "        [-0.8399],\n",
      "        [-0.7561],\n",
      "        [-0.5687],\n",
      "        [-0.2194]])\n",
      "Time step 7 Loss 9.163892078399659\n",
      "sample\n",
      "tensor([-0.6347])\n",
      "tensor([[-2.2737],\n",
      "        [-2.7498],\n",
      "        [-0.3375],\n",
      "        [ 0.0046],\n",
      "        [-0.9828],\n",
      "        [-0.8181],\n",
      "        [-1.1860],\n",
      "        [ 0.2115],\n",
      "        [-1.7439],\n",
      "        [-1.6782],\n",
      "        [-0.6997],\n",
      "        [ 0.8758],\n",
      "        [ 1.8916],\n",
      "        [ 0.4544],\n",
      "        [-0.5919],\n",
      "        [ 0.1210],\n",
      "        [ 0.1276],\n",
      "        [-0.8276],\n",
      "        [-1.3616],\n",
      "        [-3.1999],\n",
      "        [-0.3034],\n",
      "        [-0.1845],\n",
      "        [-0.0879],\n",
      "        [ 0.2737],\n",
      "        [-0.5617],\n",
      "        [ 0.3715],\n",
      "        [-1.0999],\n",
      "        [ 0.2060],\n",
      "        [ 0.5186],\n",
      "        [-0.8970],\n",
      "        [-0.6756],\n",
      "        [ 0.2795],\n",
      "        [-0.9628],\n",
      "        [-1.7330],\n",
      "        [-0.6279],\n",
      "        [-1.6230],\n",
      "        [ 0.0234],\n",
      "        [-0.9572],\n",
      "        [-2.3039],\n",
      "        [ 0.8445],\n",
      "        [-0.3329],\n",
      "        [-1.2118],\n",
      "        [ 0.4900],\n",
      "        [ 0.7920],\n",
      "        [-1.2921],\n",
      "        [-1.0907],\n",
      "        [-0.8910],\n",
      "        [-1.7460],\n",
      "        [-2.5924],\n",
      "        [-2.9770]])\n",
      "Time step 6 Loss 7.0107057094573975\n",
      "sample\n",
      "tensor([1.0982])\n",
      "tensor([[ 1.3503],\n",
      "        [ 0.3763],\n",
      "        [ 1.7817],\n",
      "        [ 0.8561],\n",
      "        [ 0.7170],\n",
      "        [ 0.3391],\n",
      "        [ 1.3724],\n",
      "        [ 0.3956],\n",
      "        [ 1.2790],\n",
      "        [ 0.6712],\n",
      "        [ 0.2769],\n",
      "        [ 0.3120],\n",
      "        [ 1.4640],\n",
      "        [ 1.5755],\n",
      "        [ 2.4501],\n",
      "        [ 1.4502],\n",
      "        [ 0.5593],\n",
      "        [ 2.8001],\n",
      "        [ 0.6088],\n",
      "        [ 1.4936],\n",
      "        [ 0.9184],\n",
      "        [ 3.0080],\n",
      "        [ 0.3215],\n",
      "        [ 0.6851],\n",
      "        [ 3.1173],\n",
      "        [ 2.1939],\n",
      "        [ 0.8480],\n",
      "        [ 1.5550],\n",
      "        [ 1.5641],\n",
      "        [ 2.3354],\n",
      "        [ 0.0288],\n",
      "        [ 0.2905],\n",
      "        [ 2.2621],\n",
      "        [ 1.9235],\n",
      "        [ 1.4925],\n",
      "        [ 1.7307],\n",
      "        [ 0.8072],\n",
      "        [-0.6862],\n",
      "        [-0.2098],\n",
      "        [ 0.3108],\n",
      "        [ 0.3921],\n",
      "        [ 2.3739],\n",
      "        [ 2.1716],\n",
      "        [ 2.0469],\n",
      "        [ 1.1168],\n",
      "        [ 0.0573],\n",
      "        [-0.2007],\n",
      "        [ 0.3542],\n",
      "        [ 0.9581],\n",
      "        [ 1.4255]])\n",
      "Time step 5 Loss 5.099319577217102\n",
      "sample\n",
      "tensor([2.0633])\n",
      "tensor([[ 1.0271],\n",
      "        [ 0.0789],\n",
      "        [ 0.9201],\n",
      "        [ 2.6201],\n",
      "        [ 1.8762],\n",
      "        [ 0.2386],\n",
      "        [ 0.3573],\n",
      "        [ 1.5830],\n",
      "        [ 3.3626],\n",
      "        [ 0.9677],\n",
      "        [ 1.2441],\n",
      "        [ 2.1684],\n",
      "        [ 3.3431],\n",
      "        [ 2.0058],\n",
      "        [ 2.1216],\n",
      "        [ 1.6144],\n",
      "        [ 1.4925],\n",
      "        [ 2.1951],\n",
      "        [ 1.1279],\n",
      "        [-0.3246],\n",
      "        [ 1.5323],\n",
      "        [ 3.4641],\n",
      "        [ 3.2422],\n",
      "        [ 1.2023],\n",
      "        [ 3.9842],\n",
      "        [ 0.5343],\n",
      "        [ 3.5268],\n",
      "        [ 1.9205],\n",
      "        [ 1.3597],\n",
      "        [ 2.8767],\n",
      "        [ 4.3799],\n",
      "        [ 2.2490],\n",
      "        [ 0.3464],\n",
      "        [ 5.1847],\n",
      "        [ 0.4117],\n",
      "        [ 1.5143],\n",
      "        [ 0.8641],\n",
      "        [ 2.6821],\n",
      "        [ 3.6965],\n",
      "        [ 0.5726],\n",
      "        [ 1.4793],\n",
      "        [ 3.2143],\n",
      "        [ 1.0857],\n",
      "        [ 2.2210],\n",
      "        [ 3.2210],\n",
      "        [ 1.6314],\n",
      "        [ 1.3007],\n",
      "        [ 3.1822],\n",
      "        [ 2.4825],\n",
      "        [ 1.2657]])\n",
      "Time step 4 Loss 3.161483019590378\n",
      "sample\n",
      "tensor([0.9906])\n",
      "tensor([[-0.4988],\n",
      "        [ 1.7195],\n",
      "        [ 2.2295],\n",
      "        [ 1.9809],\n",
      "        [ 1.1139],\n",
      "        [ 0.4297],\n",
      "        [-1.5248],\n",
      "        [ 0.7087],\n",
      "        [-0.5511],\n",
      "        [-0.5077],\n",
      "        [ 0.3692],\n",
      "        [ 1.4464],\n",
      "        [ 1.7912],\n",
      "        [ 1.1851],\n",
      "        [ 2.2803],\n",
      "        [ 2.1345],\n",
      "        [-0.1431],\n",
      "        [-0.2808],\n",
      "        [ 1.6241],\n",
      "        [-0.2165],\n",
      "        [ 0.9136],\n",
      "        [-0.4894],\n",
      "        [ 1.9378],\n",
      "        [ 1.7223],\n",
      "        [ 1.7472],\n",
      "        [ 0.9188],\n",
      "        [ 0.6501],\n",
      "        [ 0.5935],\n",
      "        [ 0.5917],\n",
      "        [ 0.6936],\n",
      "        [ 0.3528],\n",
      "        [-0.0289],\n",
      "        [ 2.1411],\n",
      "        [ 0.8714],\n",
      "        [ 0.2145],\n",
      "        [ 1.4000],\n",
      "        [ 1.0905],\n",
      "        [ 0.7947],\n",
      "        [ 0.7035],\n",
      "        [ 1.8147],\n",
      "        [ 1.5351],\n",
      "        [ 2.7930],\n",
      "        [ 0.3822],\n",
      "        [ 2.0252],\n",
      "        [ 2.1551],\n",
      "        [ 0.4870],\n",
      "        [ 1.1974],\n",
      "        [ 1.4369],\n",
      "        [ 0.1907],\n",
      "        [ 0.0986]])\n",
      "Time step 3 Loss 2.149102860689163\n",
      "sample\n",
      "tensor([2.2588])\n",
      "tensor([[3.0536],\n",
      "        [3.6763],\n",
      "        [4.1839],\n",
      "        [0.9694],\n",
      "        [0.8854],\n",
      "        [3.6789],\n",
      "        [0.0714],\n",
      "        [2.4478],\n",
      "        [2.8778],\n",
      "        [0.8101],\n",
      "        [2.3767],\n",
      "        [3.3297],\n",
      "        [3.4562],\n",
      "        [2.6688],\n",
      "        [2.8895],\n",
      "        [0.4944],\n",
      "        [1.3490],\n",
      "        [0.6623],\n",
      "        [2.0429],\n",
      "        [1.1906],\n",
      "        [1.3660],\n",
      "        [2.2030],\n",
      "        [1.2633],\n",
      "        [0.1318],\n",
      "        [3.7498],\n",
      "        [2.3833],\n",
      "        [3.1042],\n",
      "        [2.6289],\n",
      "        [1.8112],\n",
      "        [0.3166],\n",
      "        [1.3733],\n",
      "        [0.8929],\n",
      "        [3.5496],\n",
      "        [3.2880],\n",
      "        [2.4089],\n",
      "        [1.7366],\n",
      "        [1.1714],\n",
      "        [1.4538],\n",
      "        [1.3429],\n",
      "        [2.4203],\n",
      "        [1.2145],\n",
      "        [3.4152],\n",
      "        [1.3925],\n",
      "        [1.8012],\n",
      "        [1.5838],\n",
      "        [0.4616],\n",
      "        [1.3951],\n",
      "        [4.5979],\n",
      "        [1.2267],\n",
      "        [0.6975]])\n",
      "Time step 2 Loss 1.779226142168045\n",
      "sample\n",
      "tensor([2.1631])\n",
      "tensor([[ 2.4491],\n",
      "        [ 2.4806],\n",
      "        [ 2.0184],\n",
      "        [ 3.5629],\n",
      "        [ 1.6882],\n",
      "        [ 1.7657],\n",
      "        [ 1.1101],\n",
      "        [ 1.0485],\n",
      "        [ 3.3665],\n",
      "        [ 1.2514],\n",
      "        [ 1.8750],\n",
      "        [ 1.1442],\n",
      "        [ 3.0050],\n",
      "        [ 0.5991],\n",
      "        [ 2.5613],\n",
      "        [ 2.9364],\n",
      "        [ 0.0955],\n",
      "        [ 0.8050],\n",
      "        [ 1.1025],\n",
      "        [ 2.1884],\n",
      "        [ 3.3455],\n",
      "        [ 3.5369],\n",
      "        [ 2.8746],\n",
      "        [ 1.7572],\n",
      "        [ 1.3706],\n",
      "        [ 2.7898],\n",
      "        [ 1.7490],\n",
      "        [ 2.1266],\n",
      "        [ 3.1415],\n",
      "        [ 2.6245],\n",
      "        [ 1.2857],\n",
      "        [ 3.2040],\n",
      "        [ 2.0603],\n",
      "        [ 2.8680],\n",
      "        [ 3.9990],\n",
      "        [ 2.2576],\n",
      "        [ 3.1716],\n",
      "        [ 1.6459],\n",
      "        [ 2.5659],\n",
      "        [ 3.0386],\n",
      "        [ 1.1798],\n",
      "        [ 2.5006],\n",
      "        [ 2.6261],\n",
      "        [ 1.7018],\n",
      "        [ 2.7247],\n",
      "        [ 0.7410],\n",
      "        [ 2.1000],\n",
      "        [ 2.4074],\n",
      "        [ 1.5217],\n",
      "        [-0.1429]])\n",
      "Time step 1 Loss 1.3939166069030762\n",
      "sample\n",
      "tensor([1.])\n",
      "tensor([[ 0.7997],\n",
      "        [ 1.6312],\n",
      "        [ 2.6651],\n",
      "        [ 1.4587],\n",
      "        [ 2.3719],\n",
      "        [-0.3911],\n",
      "        [ 2.4372],\n",
      "        [ 1.0231],\n",
      "        [ 0.7142],\n",
      "        [ 1.7624],\n",
      "        [ 0.6522],\n",
      "        [ 0.4588],\n",
      "        [ 0.4224],\n",
      "        [ 0.2301],\n",
      "        [ 0.7755],\n",
      "        [-0.0097],\n",
      "        [ 0.9317],\n",
      "        [ 0.7695],\n",
      "        [ 2.6817],\n",
      "        [ 1.1428],\n",
      "        [ 1.2051],\n",
      "        [ 1.6333],\n",
      "        [ 1.3698],\n",
      "        [ 0.4190],\n",
      "        [-0.2587],\n",
      "        [ 0.1538],\n",
      "        [ 1.6267],\n",
      "        [ 2.0422],\n",
      "        [-0.1262],\n",
      "        [-1.0460],\n",
      "        [ 3.2234],\n",
      "        [ 2.0273],\n",
      "        [ 2.0224],\n",
      "        [ 0.4852],\n",
      "        [ 0.9651],\n",
      "        [ 1.8430],\n",
      "        [ 1.4765],\n",
      "        [ 3.6643],\n",
      "        [ 0.6440],\n",
      "        [ 0.1886],\n",
      "        [ 1.2554],\n",
      "        [-2.0915],\n",
      "        [ 2.6410],\n",
      "        [ 0.7524],\n",
      "        [ 0.2839],\n",
      "        [ 1.8986],\n",
      "        [ 1.8055],\n",
      "        [ 1.2907],\n",
      "        [ 2.3896],\n",
      "        [ 2.0246]])\n",
      "Time step 0 Loss 1.009406465291977\n",
      "Instance 0\n",
      "Last values 17.29098892211914\n",
      "Time step 8 Loss 0.084253507014364\n",
      "Time step 7 Loss 9.786761116981506\n",
      "Time step 6 Loss 7.92156765460968\n",
      "Time step 5 Loss 5.645452928543091\n",
      "Time step 4 Loss 3.7224457621574403\n",
      "Time step 3 Loss 2.363449716567993\n",
      "Time step 2 Loss 1.435028076171875\n",
      "Time step 1 Loss 1.2565078139305115\n",
      "Time step 0 Loss 1.1524020195007325\n",
      "Instance 1\n",
      "Last values 18.265186309814453\n",
      "Time step 8 Loss 0.0039014241338009015\n",
      "Time step 7 Loss 8.80155577659607\n",
      "Time step 6 Loss 6.6769685506820675\n",
      "Time step 5 Loss 5.7578761100769045\n",
      "Time step 4 Loss 4.1748828291893005\n",
      "Time step 3 Loss 2.308353579044342\n",
      "Time step 2 Loss 1.5265392750501632\n",
      "Time step 1 Loss 1.231476256251335\n",
      "Time step 0 Loss 0.9410615116357803\n",
      "Instance 2\n",
      "Last values 16.89517593383789\n",
      "Time step 8 Loss 0.0017897346740937791\n",
      "Time step 7 Loss 9.615091013908387\n",
      "Time step 6 Loss 7.723715686798096\n",
      "Time step 5 Loss 5.68202064037323\n",
      "Time step 4 Loss 4.369761025905609\n",
      "Time step 3 Loss 3.057716953754425\n",
      "Time step 2 Loss 2.1549371659755705\n",
      "Time step 1 Loss 1.5130846947431564\n",
      "Time step 0 Loss 0.7567063271999359\n",
      "Instance 3\n",
      "Last values 18.926502227783203\n",
      "Time step 8 Loss 4.4270743998708895e-05\n",
      "Time step 7 Loss 8.631566643714905\n",
      "Time step 6 Loss 6.791517734527588\n",
      "Time step 5 Loss 4.7783058166503904\n",
      "Time step 4 Loss 2.70321501493454\n",
      "Time step 3 Loss 1.8399891078472137\n",
      "Time step 2 Loss 1.3761242777109146\n",
      "Time step 1 Loss 0.9442434668540954\n",
      "Time step 0 Loss 1.0045936465263368\n",
      "Instance 4\n",
      "Last values 16.914018630981445\n",
      "Time step 8 Loss 0.001621145695571613\n",
      "Time step 7 Loss 9.576984238624572\n",
      "Time step 6 Loss 7.009783911705017\n",
      "Time step 5 Loss 6.134224677085877\n",
      "Time step 4 Loss 5.071827602386475\n",
      "Time step 3 Loss 4.144865357875824\n",
      "Time step 2 Loss 2.807450330257416\n",
      "Time step 1 Loss 2.1748349249362944\n",
      "Time step 0 Loss 1.3816481426358223\n",
      "Instance 5\n",
      "Last values 14.421308517456055\n",
      "Time step 8 Loss 0.0862499015405774\n",
      "Time step 7 Loss 9.633565664291382\n",
      "Time step 6 Loss 8.340208768844604\n",
      "Time step 5 Loss 5.926891779899597\n",
      "Time step 4 Loss 3.6931233048439025\n",
      "Time step 3 Loss 1.6806977450847627\n",
      "Time step 2 Loss 1.059107595682144\n",
      "Time step 1 Loss 1.068255078792572\n",
      "Time step 0 Loss 1.0595949023962021\n",
      "Instance 6\n",
      "Last values 15.955120086669922\n",
      "Time step 8 Loss 0.02150046700699022\n",
      "Time step 7 Loss 9.68336296081543\n",
      "Time step 6 Loss 7.352634978294373\n",
      "Time step 5 Loss 5.602147579193115\n",
      "Time step 4 Loss 3.898179030418396\n",
      "Time step 3 Loss 2.5024349629878997\n",
      "Time step 2 Loss 2.170001268386841\n",
      "Time step 1 Loss 1.6109149605035782\n",
      "Time step 0 Loss 1.2455989718437195\n",
      "Instance 7\n",
      "Last values 16.872406005859375\n",
      "Time step 8 Loss 0.0539577737217769\n",
      "Time step 7 Loss 8.959411025047302\n",
      "Time step 6 Loss 7.295167565345764\n",
      "Time step 5 Loss 5.451126885414124\n",
      "Time step 4 Loss 3.6595409750938415\n",
      "Time step 3 Loss 2.0702376276254655\n",
      "Time step 2 Loss 1.2298479855060578\n",
      "Time step 1 Loss 1.0446878492832183\n",
      "Time step 0 Loss 1.1080218493938445\n",
      "Instance 8\n",
      "Last values 17.990144729614258\n",
      "Time step 8 Loss 0.08638622853904962\n",
      "Time step 7 Loss 9.775409531593322\n",
      "Time step 6 Loss 7.466372537612915\n",
      "Time step 5 Loss 6.324609303474427\n",
      "Time step 4 Loss 4.278494453430175\n",
      "Time step 3 Loss 2.2813091039657594\n",
      "Time step 2 Loss 1.4787606447935104\n",
      "Time step 1 Loss 0.990924134850502\n",
      "Time step 0 Loss 0.9502623856067658\n",
      "Instance 9\n",
      "Last values 16.905120849609375\n",
      "All final value: [17.29098892 18.26518631 16.89517593 18.92650223 16.91401863 14.42130852\n",
      " 15.95512009 16.87240601 17.99014473 16.90512085]\n",
      "Final mean: 17.04359722137451\n",
      "Final std: 1.189253890053287\n",
      "Average time for one instance: 32.957662200927736\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import ot\n",
    "import time as Clock\n",
    "\n",
    "start = Clock.time()\n",
    "\n",
    "####### One-dimensional case #########\n",
    "# with parameter constraint\n",
    "Trunc_flag = True\n",
    "# No. of gradient descent steps (G)\n",
    "N_OPT = 20\n",
    "# No. of sample paths (N)\n",
    "smp_size = 2000\n",
    "# Sample size for empirical OT (B)\n",
    "in_sample_size = 50\n",
    "\n",
    "time_horizon = 8\n",
    "x_dim = 1\n",
    "y_dim = 1\n",
    "x_vol = 1.0\n",
    "y_vol = 0.5\n",
    "x_init = 1.0\n",
    "y_init = 2.0\n",
    "\n",
    "\n",
    "###### Multidimensional case #########\n",
    "## no parameter constraint\n",
    "# Trunc_flag = False\n",
    "# time_horizon = 5\n",
    "# x_dim = 5\n",
    "# y_dim = 5\n",
    "# x_vol = 1.1\n",
    "# y_vol = 0.1\n",
    "# x_init = 1.0\n",
    "# y_init = 2.0\n",
    "# N_OPT = 400\n",
    "# smp_size = 4000\n",
    "# in_sample_size = 300\n",
    "\n",
    "\n",
    "final_result = np.zeros(N_INSTANCE)\n",
    "\n",
    "for n_ins in range(N_INSTANCE):\n",
    "\n",
    "    val_hist = np.zeros(time_horizon+1)\n",
    "    loss_hist = np.zeros(time_horizon+1)\n",
    "\n",
    "    memory = Memory(MEM_SIZE)\n",
    "    policy_net = DQN(x_dim, y_dim, time_horizon).to(device)\n",
    "    target_net = DQN(x_dim, y_dim, time_horizon).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    # optimizer = optim.SGD(policy_net.parameters(), lr=0.1, momentum=0.9)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=1e-2) # weight_decay=1e-3)\n",
    "\n",
    "    x_path_pool = torch.zeros(smp_size, time_horizon+1, x_dim, device=device)\n",
    "    y_path_pool = torch.zeros(smp_size, time_horizon+1, y_dim, device=device)\n",
    "    x_path_pool[:, 0, :] = x_init\n",
    "    y_path_pool[:, 0, :] = y_init\n",
    "\n",
    "    for smp_id in range(smp_size):\n",
    "        # sample many paths in advance\n",
    "        for t in range(1, time_horizon + 1):\n",
    "            x_path_pool[smp_id, t, :] = x_path_pool[smp_id, t - 1, :] + x_vol * torch.randn(x_dim, device=device)\n",
    "            y_path_pool[smp_id, t, :] = y_path_pool[smp_id, t - 1, :] + y_vol * torch.randn(y_dim, device=device)\n",
    "\n",
    "    for time in range(time_horizon, -1, -1):\n",
    "\n",
    "        for smp_id in range(smp_size):\n",
    "            x_mvn = MultivariateNormal(loc=x_path_pool[smp_id, time, :], covariance_matrix=torch.eye(x_dim, device=device)*x_vol**2)\n",
    "            y_mvn = MultivariateNormal(loc=y_path_pool[smp_id, time, :], covariance_matrix=torch.eye(y_dim, device=device)*y_vol**2)\n",
    "            next_x = x_mvn.sample((in_sample_size,))\n",
    "            next_y = y_mvn.sample((in_sample_size,))\n",
    "\n",
    "            x_batch = torch.repeat_interleave(next_x, repeats=in_sample_size, dim=0)\n",
    "            y_batch = torch.tile(next_y, (in_sample_size, 1))\n",
    "            l2_mat = torch.sum((x_batch - y_batch)**2, dim=1)\n",
    "\n",
    "            if time == time_horizon:\n",
    "                expected_v = 0.0\n",
    "            elif time == time_horizon-1:\n",
    "                min_obj = l2_mat.reshape(in_sample_size, in_sample_size)\n",
    "                expected_v = ot.emd2(np.ones(in_sample_size) / in_sample_size, np.ones(in_sample_size) / in_sample_size,\n",
    "                                     min_obj.detach().cpu().numpy())\n",
    "            else:\n",
    "                val = target_net(torch.ones(x_batch.shape[0], 1, device=device)*(time+1.0), x_batch, y_batch).reshape(-1)\n",
    "                min_obj = (l2_mat + DISCOUNT*val).reshape(in_sample_size, in_sample_size)\n",
    "                expected_v = ot.emd2(np.ones(in_sample_size)/in_sample_size, np.ones(in_sample_size)/in_sample_size,\n",
    "                                     min_obj.detach().cpu().numpy())\n",
    "\n",
    "            memory.push(torch.tensor([time], dtype=torch.float32, device=device), x_path_pool[smp_id, time, :],\n",
    "                        y_path_pool[smp_id, time, :], torch.tensor([expected_v], device=device))\n",
    "\n",
    "        # Optimize at time t\n",
    "        for opt_step in range(N_OPT):\n",
    "            loss = optimize_model(policy_net, memory, optimizer, Trunc_flag)\n",
    "            if Trunc_flag:\n",
    "                with torch.no_grad():\n",
    "                    for param in policy_net.parameters():\n",
    "                        ## param.add_(torch.randn(param.size(), device=device)/50)\n",
    "                        param.clamp_(-1.0, 1.0)\n",
    "            if loss:\n",
    "                loss_hist[time] += loss.detach().cpu().item()\n",
    "\n",
    "\n",
    "        loss_hist[time] /= N_OPT\n",
    "\n",
    "        # update target network\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        # test initial value\n",
    "        val = target_net(torch.ones(1, 1, device=device)*0.0, x_path_pool[0, 0, :].reshape(1, x_dim),\n",
    "                         y_path_pool[0, 0, :].reshape(1, y_dim)).reshape(-1)\n",
    "        val_hist[time] = val\n",
    "\n",
    "        # empty memory\n",
    "        memory.clear()\n",
    "        print('Time step', time, 'Loss', loss_hist[time])\n",
    "\n",
    "        # print('Shift vector in the last layer:', target_net.linear3.bias.sum().item())\n",
    "\n",
    "\n",
    "    # for name, param in target_net.named_parameters():\n",
    "    #     if param.requires_grad:\n",
    "    #         print(name, param.data)\n",
    "\n",
    "\n",
    "    print('Instance', n_ins)\n",
    "    # print('Time elapsed', end - start)\n",
    "    print('Last values', val_hist[0])\n",
    "    final_result[n_ins] = val_hist[0]\n",
    "\n",
    "print('All final value:', final_result)\n",
    "print('Final mean:', final_result.mean())\n",
    "print('Final std:', final_result.std())\n",
    "end = Clock.time()\n",
    "print('Average time for one instance:', (end-start)/N_INSTANCE)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(val_hist)\n",
    "# plt.xlabel('Steps', fontsize=16)\n",
    "# plt.ylabel(r'$V_0$', fontsize=16)\n",
    "# # plt.tick_params(axis = 'both', which = 'major', labelsize = 16)\n",
    "# plt.legend(bbox_to_anchor=(1, 1), title='', fontsize=16, title_fontsize=16)\n",
    "# plt.savefig('conti_val.pdf', format='pdf', dpi=1000, bbox_inches='tight', pad_inches=0.1)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(loss_hist)\n",
    "# plt.xlabel('Steps', fontsize=16)\n",
    "# plt.ylabel('Loss', fontsize=16)\n",
    "# plt.savefig('conti_loss.pdf', format='pdf', dpi=1000, bbox_inches='tight', pad_inches=0.1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec2e2b-e9e2-4e7f-bb23-9b9de28706f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7dbd0b-7d50-4b20-bb00-b0a268973091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
